{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v4Eqk5U7Xn60"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KtRg9NjOXqXi"
      },
      "outputs": [],
      "source": [
        "# os.chdir('drive/My Drive/')  # Change this path to your desired directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yc73BD2NXuxE"
      },
      "outputs": [],
      "source": [
        "# ! pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NyEt-OTgYY9H"
      },
      "outputs": [],
      "source": [
        "# # Upload your kaggle.json API key file to your Colab environment.\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Move the kaggle.json file to the appropriate directory and set the correct permissions.\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !mv kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YvwreVw8YGQV"
      },
      "outputs": [],
      "source": [
        "# !kaggle datasets download -d ishanikathuria/handwritten-signature-datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ujdS9SbOZlBC"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "\n",
        "# # Replace 'dataset.zip' with the actual name of your zip file\n",
        "# with zipfile.ZipFile('handwritten-signature-datasets.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('/content/dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1HQ-_RSplA5h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "a=tf.config.experimental.list_physical_devices(\"GPU\")\n",
        "if a :\n",
        "  for b in a:\n",
        "    tf.config.experimental.set_memory_growth(b,True)\n",
        "  device=a[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eR79MlxVT-5",
        "outputId": "7d2206fe-da28-451b-d0dd-6252c9c2dc74"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import random\n",
        "# from itertools import combinations\n",
        "# import numpy as np\n",
        "\n",
        "# # # Mount Google Drive (if your dataset is stored there)\n",
        "# # from google.colab import drive\n",
        "\n",
        "# # drive.flush_and_unmount()\n",
        "\n",
        "# # drive.mount('/dataset/CEDAR/')\n",
        "\n",
        "# # os.chdir('../../')  # Change this path to your desired directory\n",
        "\n",
        "\n",
        "# # Define the path to your dataset folder on Google Drive\n",
        "# dataset_folder = '/content/dataset/CEDAR/CEDAR'  # Adjust the path to your dataset\n",
        "\n",
        "# # Check if the dataset folder exists\n",
        "# if not os.path.exists(dataset_folder):\n",
        "#     print(f\"The dataset folder '{dataset_folder}' does not exist.\")\n",
        "# else:\n",
        "#     print(f\"Dataset folder '{dataset_folder}' found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gxSM8tC3UkaV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "\n",
        "# Define the path to your dataset folder\n",
        "dataset_folder = 'archive\\CEDAR\\CEDAR'\n",
        "\n",
        "# Define the percentage of data for the training set\n",
        "train_percentage = 0.7\n",
        "\n",
        "# Initialize lists to store image pairs and their labels\n",
        "train_image_pairs = []\n",
        "train_labels = []\n",
        "test_image_pairs = []\n",
        "test_labels = []\n",
        "\n",
        "# Iterate through each folder in the dataset\n",
        "for folder_name in os.listdir(dataset_folder):\n",
        "    folder_path = os.path.join(dataset_folder, folder_name)\n",
        "\n",
        "    # Create a list of original and forged images in the folder\n",
        "    original_images = []\n",
        "    forged_images = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.startswith('original'):\n",
        "            original_images.append(file_name)\n",
        "        elif file_name.startswith('forgeries'):\n",
        "            forged_images.append(file_name)\n",
        "\n",
        "    # Shuffle the original and forged images to ensure randomness\n",
        "    random.shuffle(original_images)\n",
        "    random.shuffle(forged_images)\n",
        "\n",
        "    # # Determine how many images to use for training and testing based on the percentage\n",
        "    # num_train_original = int(train_percentage * len(original_images))\n",
        "    # num_test_original = len(original_images) - num_train_original\n",
        "    # num_train_forged = int(train_percentage * len(forged_images))\n",
        "    # num_test_forged = len(forged_images) - num_train_forged\n",
        "\n",
        "    # Generate pairs for original-original and original-forged combinations\n",
        "    original_original_pairs = list(combinations(original_images, 2))\n",
        "    original_forged_pairs = [(original_image, forged_image) for original_image in original_images for forged_image in forged_images]\n",
        "\n",
        "    # Shuffle the pairs for randomness\n",
        "    random.shuffle(original_original_pairs)\n",
        "    random.shuffle(original_forged_pairs)\n",
        "\n",
        "    # Select an equal number of original-original and original-forged pairs for training\n",
        "    original_original_pairs = original_original_pairs[:50]\n",
        "    original_forged_pairs = original_forged_pairs[:len(original_original_pairs)]\n",
        "\n",
        "\n",
        "    train_number=int(train_percentage*len(original_original_pairs))\n",
        "\n",
        "    train_original_original_pairs=original_original_pairs[:train_number]\n",
        "    train_original_forged_pairs=original_forged_pairs[:train_number]\n",
        "    # Assign labels (0 for original-original and 1 for original-forged)\n",
        "    train_labels.extend([0] * len(train_original_original_pairs) + [1] * len(train_original_forged_pairs))\n",
        "\n",
        "\n",
        "    # Extend the training sets\n",
        "    train_image_pairs.extend([(os.path.join(folder_path, pair[0]), os.path.join(folder_path, pair[1])) for pair in train_original_original_pairs + train_original_forged_pairs])\n",
        "\n",
        "    # Select the remaining pairs for testing\n",
        "    test_original_original_pairs = original_original_pairs[train_number:]\n",
        "    test_original_forged_pairs = original_forged_pairs[train_number:]\n",
        "\n",
        "    # Assign labels for testing\n",
        "    test_labels.extend([0] * len(test_original_original_pairs) + [1] * len(test_original_forged_pairs))\n",
        "\n",
        "    # Extend the testing sets\n",
        "    test_image_pairs.extend([(os.path.join(folder_path, pair[0]), os.path.join(folder_path, pair[1])) for pair in test_original_original_pairs + test_original_forged_pairs])\n",
        "\n",
        "# Now you have the training and testing sets with image pairs and labels as per your requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bEFeQqgUpu6",
        "outputId": "bd46aa87-19a8-4718-f87d-0feaaa984a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample test_image_pairs:\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_9.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_16.png') Label: 0\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_4.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_13.png') Label: 0\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_20.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_7.png') Label: 0\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_21.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_17.png') Label: 0\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_24.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_14.png') Label: 0\n",
            "Number of test_image_pairs: 1650\n",
            "Number of test_labels: 1650\n",
            "\n",
            "Sample train_image_pairs:\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_13.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_12.png') Label: 0\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_24.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_8.png') Label: 0\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_23.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_3.png') Label: 0\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_8.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_11.png') Label: 0\n",
            "('archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_24.png', 'archive\\\\CEDAR\\\\CEDAR\\\\1\\\\original_1_23.png') Label: 0\n",
            "Number of train_image_pairs: 3850\n",
            "Number of train_labels: 3850\n"
          ]
        }
      ],
      "source": [
        "# Print a few examples from test_image_pairs and test_labels\n",
        "print(\"Sample test_image_pairs:\")\n",
        "for i in range(min(5, len(test_image_pairs))):  # Print up to 5 examples\n",
        "    print(test_image_pairs[i], \"Label:\", test_labels[i])\n",
        "\n",
        "# Print the total number of test_image_pairs and test_labels\n",
        "print(\"Number of test_image_pairs:\", len(test_image_pairs))\n",
        "print(\"Number of test_labels:\", len(test_labels))\n",
        "\n",
        "# Print a few examples from train_image_pairs and train_labels\n",
        "print(\"\\nSample train_image_pairs:\")\n",
        "for i in range(min(5, len(train_image_pairs))):  # Print up to 5 examples\n",
        "    print(train_image_pairs[i], \"Label:\", train_labels[i])\n",
        "\n",
        "# Print the total number of train_image_pairs and train_labels\n",
        "print(\"Number of train_image_pairs:\", len(train_image_pairs))\n",
        "print(\"Number of train_labels:\", len(train_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ-eomEvUsvG",
        "outputId": "c2c37bdc-bcf6-4945-f0e5-626aa5c025d8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.applications import VGG19\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define a function to preprocess the image pairs\n",
        "def preprocess_image(image_path):\n",
        "    # Load, resize, and normalize the image\n",
        "    image = keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "    image = keras.preprocessing.image.img_to_array(image) / 255.0\n",
        "    return image\n",
        "\n",
        "# Load the pre-trained VGG-19 model without the top classification layers\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the first 5 layers\n",
        "for layer in base_model.layers[:5]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the Siamese CNN model\n",
        "def create_siamese_model(input_shape):\n",
        "    # Create the left and right inputs\n",
        "    input_left = keras.layers.Input(shape=input_shape)\n",
        "    input_right = keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # Generate the embeddings for the left and right inputs using the base VGG-19 model\n",
        "    embedding_left = base_model(input_left)\n",
        "    embedding_right = base_model(input_right)\n",
        "\n",
        "    # Flatten the embeddings\n",
        "    embedding_left = keras.layers.Flatten()(embedding_left)\n",
        "    embedding_right = keras.layers.Flatten()(embedding_right)\n",
        "\n",
        "    # Calculate the L1 distance between the embeddings\n",
        "    L1_distance = keras.layers.Lambda(lambda embeddings: tf.abs(embeddings[0] - embeddings[1]))([embedding_left, embedding_right])\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = keras.layers.Dense(1, activation='sigmoid')(L1_distance)\n",
        "\n",
        "    # Create the Siamese model\n",
        "    siamese_model = keras.models.Model(inputs=[input_left, input_right], outputs=output_layer)\n",
        "\n",
        "    return siamese_model\n",
        "\n",
        "# Create and compile the Siamese model\n",
        "input_shape = (224, 224, 3)\n",
        "siamese_model = create_siamese_model(input_shape)\n",
        "siamese_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# The rest of your code for loading data and training the model remains the same.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Convert your image pairs to NumPy arrays\n",
        "train_image_pairs_left = np.array([preprocess_image(pair[0]) for pair in train_image_pairs])\n",
        "train_image_pairs_right = np.array([preprocess_image(pair[1]) for pair in train_image_pairs])\n",
        "\n",
        "# Convert your labels to a NumPy array\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Similarly, preprocess and convert test image pairs and labels\n",
        "test_image_pairs_left = np.array([preprocess_image(pair[0]) for pair in test_image_pairs])\n",
        "test_image_pairs_right = np.array([preprocess_image(pair[1]) for pair in test_image_pairs])\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n",
        "\n",
        "# Train the Siamese model\n",
        "siamese_model.fit(\n",
        "    [train_image_pairs_left, train_image_pairs_right],\n",
        "    train_labels,\n",
        "    batch_size=8,\n",
        "    epochs=10,\n",
        "    validation_data=([test_image_pairs_left, test_image_pairs_right], test_labels),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Save the Siamese model\n",
        "siamese_model.save('vgg_19')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxsqNcVqb3p4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiDEeKXShiJs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
